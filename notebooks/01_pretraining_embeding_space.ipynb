{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6497f4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.temporal_autoencoder import pretrain_en_de_with_regularizers\n",
    "\n",
    "# Example usage: pretrain embeddings on temporal tensor\n",
    "# (Assumes temporal_node_tensor and masks are already defined above)\n",
    "model, logs, best_epoch = pretrain_en_de_with_regularizers(\n",
    "    train_tensor=temporal_node_tensor,\n",
    "    val_tensor=None,\n",
    "    in_dim=masks[\"mask_embed\"].sum().item(),\n",
    "    embed_dim=16,\n",
    "    conv_hidden=128,\n",
    "    window=6,\n",
    "    use_attention=True,\n",
    "    batch_size=8,\n",
    "    lr=1e-3,\n",
    "    epochs=20,\n",
    "    device=device,\n",
    "    early_stopping_patience=5,\n",
    "    mask_embed=masks[\"mask_embed\"],\n",
    "    mask_cloud=masks[\"mask_cloud\"],\n",
    "    save_path=None,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e579f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.cv_splits import make_expanding_folds, extract_fold_data\n",
    "\n",
    "T = temporal_node_tensor.shape[0]\n",
    "first_train_end = int(T * 0.33)\n",
    "val_window = int(T * 0.11)\n",
    "n_folds = 3\n",
    "\n",
    "folds = make_expanding_folds(\n",
    "    T=T,\n",
    "    train_start=0,\n",
    "    first_train_end=first_train_end,\n",
    "    val_window=val_window,\n",
    "    n_folds=n_folds,\n",
    ")\n",
    "\n",
    "print(\"FOLD SPLITS:\")\n",
    "for i, f in enumerate(folds):\n",
    "    print(f\"Fold {i+1}: Train {f['train_slice']}  Val {f['val_slice']}\")\n",
    "\n",
    "train_data, val_data = extract_fold_data(temporal_node_tensor, folds, fold_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4968a2e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.hparam_search_encoder import HParamConfig, run_hparam_search, select_best\n",
    "\n",
    "cfg = HParamConfig(\n",
    "    embed_dims=[8, 16, 32],\n",
    "    conv_hiddens=[32, 64, 128, 256],\n",
    "    seeds=[123],\n",
    "    folds=[0, 1, 2],\n",
    "    window=12,\n",
    "    use_attention=True,\n",
    "    batch_size=8,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    early_stopping_patience=5,\n",
    ")\n",
    "\n",
    "results = run_hparam_search(\n",
    "    temporal_node_tensor=temporal_node_tensor,\n",
    "    folds=folds,\n",
    "    mask_embed=masks[\"mask_embed\"].bool().to(device),\n",
    "    mask_cloud=masks[\"mask_cloud\"].bool().to(device),\n",
    "    base_dir=\"/content/gdrive/MyDrive/hparam_search_encoder\",\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "best_cfg = select_best(results)\n",
    "print(best_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7c19a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.hparam_results import collect_hparam_records, summarize_hparam_results\n",
    "\n",
    "base_dir = \"/content/gdrive/MyDrive/hparam_search_encoder\"\n",
    "df = collect_hparam_records(base_dir)\n",
    "summary = summarize_hparam_results(df)\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
