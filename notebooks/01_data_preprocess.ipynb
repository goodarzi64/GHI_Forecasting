{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576d848",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Colab mount ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# --- Paths (make meaning explicit) ---\n",
    "geo_dir = '/content/gdrive/MyDrive/CYL_geo'\n",
    "raw_prep_dir = '/content/gdrive/MyDrive/CYL_GHI/prep_files'   # where *_prep.csv files live\n",
    "artifacts_dir = '/content/gdrive/MyDrive/CYL_GHI/prep_files'  # where *.npz artifacts live (can be same)\n",
    "\n",
    "# ================================================================\n",
    "# 1) Stations + static geo features\n",
    "# ================================================================\n",
    "stations_csv = os.path.join(geo_dir, 'stations.csv')\n",
    "df_geo = pd.read_csv(stations_csv)\n",
    "station_files = df_geo['station_code'].tolist()\n",
    "\n",
    "vars_geo = df_geo[[\"height\", \"Slope_DEM2_U1\", \"Aspect_DEM2_1\", \"rastercalc\"]].rename(\n",
    "    columns={\"Slope_DEM2_U1\": \"slope\", \"Aspect_DEM2_1\": \"aspect\", \"rastercalc\": \"twi\"}\n",
    ")\n",
    "\n",
    "aspect_rad = np.deg2rad(vars_geo[\"aspect\"].to_numpy())\n",
    "vars_geo[\"aspect_cos\"] = np.cos(aspect_rad)\n",
    "vars_geo[\"aspect_sin\"] = np.sin(aspect_rad)\n",
    "vars_geo = vars_geo.drop(columns=[\"aspect\"])\n",
    "\n",
    "geo_scaler = MinMaxScaler()\n",
    "vars_geo_scaled = geo_scaler.fit_transform(vars_geo.to_numpy())\n",
    "vars_geo = pd.DataFrame(vars_geo_scaled, columns=vars_geo.columns)\n",
    "\n",
    "# ================================================================\n",
    "# 2) Load or build temporal tensors\n",
    "# ================================================================\n",
    "loading_flag = True  # False => rebuild preprocessing\n",
    "\n",
    "if loading_flag:\n",
    "    files_to_load = {\n",
    "        \"node_tensor\": os.path.join(artifacts_dir, \"node_tensor2.npz\"),\n",
    "        \"target_tensor\": os.path.join(artifacts_dir, \"target_tensor2.npz\"),\n",
    "        \"columns\": os.path.join(artifacts_dir, \"columns2.npz\"),\n",
    "        \"masks\": os.path.join(artifacts_dir, \"masks2.npz\"),\n",
    "    }\n",
    "\n",
    "    loaded = {}\n",
    "    for name, path in files_to_load.items():\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Missing required artifact: {path}\")\n",
    "        loaded[name] = np.load(path, allow_pickle=True)\n",
    "        print(f\"✅ Loaded: {name}\")\n",
    "\n",
    "    temporal_node_tensor = loaded[\"node_tensor\"][\"data\"]\n",
    "    temporal_target_tensor = loaded[\"target_tensor\"][\"data\"]\n",
    "    df_cols = loaded[\"columns\"][\"data\"]\n",
    "\n",
    "    masks_file = loaded[\"masks\"]\n",
    "    masks = {k: masks_file[k] for k in masks_file.files}\n",
    "\n",
    "else:\n",
    "    df_list, target_list = [], []\n",
    "    df_cols_ref = None\n",
    "\n",
    "    for station in station_files:\n",
    "        path = os.path.join(raw_prep_dir, f\"{station}_prep.csv\")\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        # --- Time encodings ---\n",
    "        ts = pd.to_datetime(df.iloc[:, 0])\n",
    "        doy = ts.dt.dayofyear.to_numpy()\n",
    "        tod = (ts.dt.hour * 3600 + ts.dt.minute * 60 + ts.dt.second).to_numpy()\n",
    "\n",
    "        df = df.assign(\n",
    "            doy_sin=np.sin(2 * np.pi * doy / 365.0),\n",
    "            doy_cos=np.cos(2 * np.pi * doy / 365.0),\n",
    "            tod_sin=np.sin(2 * np.pi * tod / 86400.0),\n",
    "            tod_cos=np.cos(2 * np.pi * tod / 86400.0),\n",
    "            wind_dir_sin=lambda x: np.sin(np.radians(x[\"wind_dir\"])),\n",
    "            wind_dir_cos=lambda x: np.cos(np.radians(x[\"wind_dir\"])),\n",
    "            sun_azim_sin=lambda x: np.sin(np.radians(x[\"sun_azim\"])),\n",
    "            sun_azim_cos=lambda x: np.cos(np.radians(x[\"sun_azim\"])),\n",
    "        )\n",
    "\n",
    "        # Drop raw azimuth but keep wind_dir last\n",
    "        if \"sun_azim\" in df.columns:\n",
    "            df = df.drop(columns=[\"sun_azim\"])\n",
    "        cols = [c for c in df.columns if c != \"wind_dir\"] + [\"wind_dir\"]\n",
    "        df = df[cols]\n",
    "\n",
    "        features = df.iloc[:, 1:]  # drop timestamp\n",
    "        target = df[\"GHI\"].to_numpy()\n",
    "\n",
    "        # Column consistency check\n",
    "        if df_cols_ref is None:\n",
    "            df_cols_ref = features.columns.to_list()\n",
    "        else:\n",
    "            if features.columns.to_list() != df_cols_ref:\n",
    "                raise ValueError(f\"Column mismatch at station {station}. Check *_prep.csv consistency.\")\n",
    "\n",
    "        # Force numeric dtype (prevents object arrays)\n",
    "        df_list.append(features.to_numpy(dtype=np.float32))\n",
    "        target_list.append(target.astype(np.float32))\n",
    "\n",
    "    df_cols = np.array(df_cols_ref, dtype=object)\n",
    "    temporal_node_tensor = np.stack(df_list, axis=1)         # (T, N, F)\n",
    "    temporal_target_tensor = np.stack(target_list, axis=1)   # (T, N)\n",
    "\n",
    "    # --- Normalize selected input features ---\n",
    "    non_norm_features = [\n",
    "        \"NDVI\",\n",
    "        *[c for c in df_cols if str(c).startswith(\"cloud_\")],\n",
    "        \"wind_dir\",  # keep raw wind_dir unscaled\n",
    "    ]\n",
    "\n",
    "    norm_mask = np.array([c not in non_norm_features for c in df_cols], dtype=bool)\n",
    "    notnorm_mask = ~norm_mask\n",
    "\n",
    "    norm_node = temporal_node_tensor[:, :, norm_mask]\n",
    "    notnorm_node = temporal_node_tensor[:, :, notnorm_mask]\n",
    "\n",
    "    T, N, F_norm = norm_node.shape\n",
    "    scaler = MinMaxScaler()\n",
    "    norm_2d = norm_node.reshape(-1, F_norm)\n",
    "    norm_2d_scaled = scaler.fit_transform(norm_2d)\n",
    "    norm_node_scaled = norm_2d_scaled.reshape(T, N, F_norm).astype(np.float32)\n",
    "\n",
    "    temporal_node_tensor_scaled = np.zeros_like(temporal_node_tensor, dtype=np.float32)\n",
    "    temporal_node_tensor_scaled[:, :, norm_mask] = norm_node_scaled\n",
    "    temporal_node_tensor_scaled[:, :, notnorm_mask] = notnorm_node.astype(np.float32)\n",
    "    temporal_node_tensor = temporal_node_tensor_scaled\n",
    "\n",
    "    # --- Masks ---\n",
    "    F = len(df_cols)\n",
    "    mask_gate = np.zeros(F, dtype=bool)\n",
    "    mask_wind = np.zeros(F, dtype=bool)\n",
    "    mask_embed = np.zeros(F, dtype=bool)\n",
    "    mask_forecast = np.zeros(F, dtype=bool)\n",
    "    mask_cloud = np.zeros(F, dtype=bool)\n",
    "\n",
    "    col_idx = {col: i for i, col in enumerate(df_cols)}\n",
    "\n",
    "    for key in [\n",
    "        \"GHI\",\"humidity\",\"precipitation\",\"air_temp\",\"sun_elev\",\"AOD\",\n",
    "        \"C_GHI\",\"Dew_Point\",\"S_Albedo\",\"Pressure\",\"sun_azim_sin\",\"sun_azim_cos\",\n",
    "        \"cloud_Clear\",\"cloud_Probably_Clear\",\"cloud_Water\",\"cloud_Super-Cooled_Water\",\n",
    "        \"cloud_Mixed\",\"cloud_Opaque_Ice\",\"cloud_Cirrus\",\"cloud_Overlapping\",\"cloud_Overshooting\",\n",
    "        \"NDVI\",\"toa\",\"wind_sp\"\n",
    "    ]:\n",
    "        if key in col_idx:\n",
    "            mask_gate[col_idx[key]] = True\n",
    "\n",
    "    for key in [\"wind_dir\", \"wind_sp\"]:\n",
    "        if key in col_idx:\n",
    "            mask_wind[col_idx[key]] = True\n",
    "\n",
    "    for key in [\n",
    "        \"GHI\",\"humidity\",\"precipitation\",\"air_temp\",\"sun_elev\",\"AOD\",\n",
    "        \"C_GHI\",\"Dew_Point\",\"S_Albedo\",\"Pressure\",\"sun_azim_sin\",\"sun_azim_cos\",\n",
    "        \"cloud_Clear\",\"cloud_Probably_Clear\",\"cloud_Water\",\"cloud_Super-Cooled_Water\",\n",
    "        \"cloud_Mixed\",\"cloud_Opaque_Ice\",\"cloud_Cirrus\",\"cloud_Overlapping\",\"cloud_Overshooting\"\n",
    "    ]:\n",
    "        if key in col_idx:\n",
    "            mask_embed[col_idx[key]] = True\n",
    "\n",
    "    for key in [\n",
    "        \"cloud_Clear\",\"cloud_Probably_Clear\",\"cloud_Water\",\"cloud_Super-Cooled_Water\",\n",
    "        \"cloud_Mixed\",\"cloud_Opaque_Ice\",\"cloud_Cirrus\",\"cloud_Overlapping\",\"cloud_Overshooting\"\n",
    "    ]:\n",
    "        if key in col_idx:\n",
    "            mask_cloud[col_idx[key]] = True\n",
    "\n",
    "    for key in list(col_idx.keys()):\n",
    "        if key not in [\"wind_dir\", \"toa\", \"NDVI\"]:\n",
    "            mask_forecast[col_idx[key]] = True\n",
    "\n",
    "    masks = {\n",
    "        \"mask_gate\": mask_gate,\n",
    "        \"mask_wind\": mask_wind,\n",
    "        \"mask_embed\": mask_embed,\n",
    "        \"mask_forecast\": mask_forecast,\n",
    "        \"mask_cloud\": mask_cloud,\n",
    "    }\n",
    "\n",
    "    # --- Save artifacts ---\n",
    "    np.savez_compressed(os.path.join(artifacts_dir, \"node_tensor2.npz\"), data=temporal_node_tensor)\n",
    "    np.savez_compressed(os.path.join(artifacts_dir, \"target_tensor2.npz\"), data=temporal_target_tensor)\n",
    "    np.savez_compressed(os.path.join(artifacts_dir, \"columns2.npz\"), data=df_cols)\n",
    "    np.savez_compressed(os.path.join(artifacts_dir, \"masks2.npz\"), **masks)\n",
    "    print(\"✅ Preprocessing complete and saved.\")\n",
    "\n",
    "# ================================================================\n",
    "# 3) Wind positions\n",
    "# ================================================================\n",
    "wind_cols = df_cols[masks[\"mask_wind\"]].tolist()\n",
    "if \"wind_dir\" not in wind_cols or \"wind_sp\" not in wind_cols:\n",
    "    raise KeyError(\"wind_dir and/or wind_sp not found within mask_wind-selected columns.\")\n",
    "wind_dir_pos = wind_cols.index(\"wind_dir\")\n",
    "wind_sp_pos = wind_cols.index(\"wind_sp\")\n",
    "print(\"wind_dir_pos:\", wind_dir_pos, \"wind_sp_pos:\", wind_sp_pos)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
